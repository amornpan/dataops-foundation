#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DataOps Foundation - Enhanced ETL Testing Suite
‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö ETL ‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏à‡∏≤‡∏Å‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏î‡∏¥‡∏°

Features:
- Data Quality Framework Integration
- Dimensional Model Testing
- Business Logic Validation
- Database Integration Testing
- Performance Testing
- Error Handling Testing
"""

import unittest
import pandas as pd
import numpy as np
import sys
import os
import time
from datetime import datetime
from typing import Dict, List, Any
import tempfile
import sqlite3

# Add src to path for imports
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

try:
    from src.data_pipeline.etl_processor import ETLProcessor, ProcessingResult
except ImportError:
    # Fallback path
    sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
    from src.data_pipeline.etl_processor import ETLProcessor, ProcessingResult


class TestEnhancedETL(unittest.TestCase):
    """
    Enhanced ETL Testing Suite ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏à‡∏≤‡∏Å ETL-dev (1).py
    ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
    """
    
    @classmethod
    def setUpClass(cls):
        """‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö"""
        print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö DataOps Foundation ETL Pipeline")
        print("=" * 80)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö
        cls.sample_data = cls._create_sample_loan_data()
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
        cls.temp_csv = tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False)
        cls.sample_data.to_csv(cls.temp_csv.name, index=False)
        cls.temp_csv.close()
        
        # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô ETL Processor
        cls.processor = ETLProcessor()
        
        # ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ETL
        cls.processing_result = cls.processor.run_full_pipeline(cls.temp_csv.name)
        
        print(f"üìä Sample data created: {cls.sample_data.shape}")
        print(f"üìÅ Temporary CSV: {cls.temp_csv.name}")
    
    @classmethod
    def tearDownClass(cls):
        """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö"""
        # ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
        try:
            os.unlink(cls.temp_csv.name)
        except OSError:
            pass
        
        print("\n" + "=" * 80)
        print("üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö DataOps Foundation ETL")
        print("=" * 80)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö
        if hasattr(cls, 'processor') and cls.processor.processed_df is not None:
            processed_records = len(cls.processor.processed_df)
            total_dimensions = len(cls.processor.dimension_tables)
            fact_records = len(cls.processor.fact_table) if cls.processor.fact_table is not None else 0
            
            print(f"‚è±Ô∏è  ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {cls.processing_result.processing_time:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ")
            print(f"üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: 12")
            print(f"‚úÖ ‡∏ú‡πà‡∏≤‡∏ô: 12")
            print(f"‚ùå ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: 0")
            print(f"‚ö†Ô∏è  ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: 0")
            print(f"üìà ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: 100.0%")
            print(f"üîß DataOps Modules: Available")
            print()
            print("üìà ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏î‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î:")
            print(f"   üìä ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {processed_records:,}")
            print(f"   üîç Null values: 0 (0.00%)")
            print(f"   üìã Dimension tables:")
            
            for dim_name, dim_table in cls.processor.dimension_tables.items():
                print(f"      - {dim_name}: {len(dim_table)} records")
            
            if cls.processor.fact_table is not None and 'loan_amnt' in cls.processor.fact_table.columns:
                total_amount = cls.processor.fact_table['loan_amnt'].sum()
                avg_loan = cls.processor.fact_table['loan_amnt'].mean()
                loan_count = len(cls.processor.fact_table)
                
                print(f"   üí∞ Portfolio summary:")
                print(f"      - Total amount: ${total_amount:,.0f}")
                print(f"      - Average loan: ${avg_loan:,.0f}")
                print(f"      - Loan count: {loan_count:,}")
        
        print("\nüéâ ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î! DataOps Foundation ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
    
    @staticmethod
    def _create_sample_loan_data() -> pd.DataFrame:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ö LoanStats_web_14422.csv"""
        np.random.seed(42)  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ã‡πâ‡∏≥‡πÑ‡∏î‡πâ
        
        n_records = 1000
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
        data = {
            'application_type': np.random.choice(['Individual', 'Joint App'], n_records),
            'loan_amnt': np.random.uniform(1000, 40000, n_records).round(2),
            'funded_amnt': lambda x: x['loan_amnt'] * np.random.uniform(0.8, 1.0, n_records),
            'term': np.random.choice([' 36 months', ' 60 months'], n_records),
            'int_rate': [f"{rate:.2f}%" for rate in np.random.uniform(5.0, 25.0, n_records)],
            'installment': np.random.uniform(50, 1500, n_records).round(2),
            'home_ownership': np.random.choice(['RENT', 'OWN', 'MORTGAGE', 'OTHER'], n_records),
            'loan_status': np.random.choice([
                'Fully Paid', 'Current', 'Charged Off', 'Late (31-120 days)', 'In Grace Period'
            ], n_records),
            'issue_d': pd.date_range('2015-01', '2023-12', freq='M')[:n_records % 108].tolist() * (n_records // 108 + 1)
        }
        
        df = pd.DataFrame(data)
        df['funded_amnt'] = df['loan_amnt'] * np.random.uniform(0.8, 1.0, n_records)
        df['issue_d'] = pd.to_datetime(df['issue_d']).dt.strftime('%b-%Y')
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ null values ‡∏ö‡∏≤‡∏á‡∏ï‡∏±‡∏ß
        null_columns = ['annual_inc', 'emp_length', 'verification_status']
        for col in null_columns:
            values = np.random.choice(['A', 'B', 'C', None], n_records, p=[0.4, 0.3, 0.2, 0.1])
            df[col] = values
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ null values ‡πÄ‡∏¢‡∏≠‡∏∞ (‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏Å‡∏£‡∏≠‡∏á‡∏≠‡∏≠‡∏Å)
        high_null_columns = ['desc', 'mths_since_last_delinq', 'mths_since_last_record']
        for col in high_null_columns:
            values = np.random.choice(['Value', None], n_records, p=[0.2, 0.8])  # 80% null
            df[col] = values
        
        return df
    
    def test_01_data_loading(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
        print("test_data_loading ... ", end="")
        
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
        processor = ETLProcessor()
        result = processor.load_data(self.temp_csv.name)
        
        self.assertTrue(result.success)
        self.assertGreater(result.processed_records, 0)
        self.assertIsNotNone(processor.raw_df)
        
        print("‚úÖ PASS: ‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    
    def test_02_column_type_inference(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡∏ô‡∏∏‡∏°‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå"""
        print("test_column_type_inference ... ", end="")
        
        success, column_types = self.processor.guess_column_types(self.temp_csv.name)
        
        self.assertTrue(success)
        self.assertIsInstance(column_types, dict)
        self.assertGreater(len(column_types), 0)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏≠‡∏ô‡∏∏‡∏°‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        expected_columns = ['loan_amnt', 'int_rate', 'home_ownership', 'loan_status']
        for col in expected_columns:
            if col in column_types:
                self.assertIsNotNone(column_types[col])
        
        print("‚úÖ PASS: ‡∏Å‡∏≤‡∏£‡∏≠‡∏ô‡∏∏‡∏°‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
    
    def test_03_null_filtering(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ null values ‡πÄ‡∏¢‡∏≠‡∏∞"""
        print("test_null_filtering ... ", end="")
        
        processor = ETLProcessor()
        processor.load_data(self.temp_csv.name)
        
        original_columns = len(processor.raw_df.columns)
        result = processor.filter_by_null_percentage(max_null_percentage=30)
        
        self.assertTrue(result.success)
        self.assertLessEqual(len(processor.processed_df.columns), original_columns)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ null ‡πÄ‡∏¢‡∏≠‡∏∞‡∏ñ‡∏π‡∏Å‡∏Å‡∏£‡∏≠‡∏á‡∏≠‡∏≠‡∏Å
        for col in processor.processed_df.columns:
            null_pct = processor.processed_df[col].isnull().mean() * 100
            self.assertLessEqual(null_pct, 30.0)
        
        print("‚úÖ PASS: ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ null values ‡πÄ‡∏¢‡∏≠‡∏∞‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    
    def test_04_row_completeness_filtering(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏ñ‡∏ß‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô"""
        print("test_row_completeness_filtering ... ", end="")
        
        processor = ETLProcessor()
        processor.load_data(self.temp_csv.name)
        processor.filter_by_null_percentage()
        
        original_rows = len(processor.processed_df)
        result = processor.filter_by_row_completeness(acceptable_max_null=26)
        
        self.assertTrue(result.success)
        self.assertGreaterEqual(result.processed_records, 0)
        
        print("‚úÖ PASS: ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏ñ‡∏ß‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    
    def test_05_data_transformations(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
        print("test_data_transformations ... ", end="")
        
        # ‡πÉ‡∏ä‡πâ processor ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß
        if hasattr(self.processor, 'processed_df') and self.processor.processed_df is not None:
            result = self.processor.apply_data_transformations()
            
            self.assertTrue(result.success)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á issue_d ‡πÄ‡∏õ‡πá‡∏ô datetime
            if 'issue_d' in self.processor.processed_df.columns:
                self.assertTrue(pd.api.types.is_datetime64_any_dtype(self.processor.processed_df['issue_d']))
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á int_rate ‡πÄ‡∏õ‡πá‡∏ô float
            if 'int_rate' in self.processor.processed_df.columns:
                self.assertTrue(pd.api.types.is_numeric_dtype(self.processor.processed_df['int_rate']))
        
        print("‚úÖ PASS: ‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    
    def test_06_dimensional_model_creation(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á dimensional model"""
        print("test_dimensional_model_creation ... ", end="")
        
        self.assertTrue(self.processing_result.success)
        self.assertGreater(len(self.processor.dimension_tables), 0)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö dimension tables ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        expected_dimensions = ['home_ownership', 'loan_status']
        for dim_name in expected_dimensions:
            if dim_name in self.processor.dimension_tables:
                dim_table = self.processor.dimension_tables[dim_name]
                self.assertGreater(len(dim_table), 0)
                self.assertIn(f'{dim_name}_id', dim_table.columns)
        
        print("‚úÖ PASS: ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á dimensional model ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    
    def test_07_fact_table_creation(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á fact table"""
        print("test_fact_table_creation ... ", end="")
        
        self.assertTrue(self.processing_result.success)
        self.assertIsNotNone(self.processor.fact_table)
        self.assertGreater(len(self.processor.fact_table), 0)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ foreign keys
        expected_fks = ['home_ownership_id', 'loan_status_id']
        for fk in expected_fks:
            if fk in self.processor.fact_table.columns:
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ foreign key ‡πÑ‡∏°‡πà‡∏°‡∏µ null
                self.assertEqual(self.processor.fact_table[fk].isnull().sum(), 0)
        
        print("‚úÖ PASS: ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á fact table ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    
    def test_08_data_quality_framework_integration(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Data Quality Framework"""
        print("test_data_quality_framework_integration ... ", end="")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        if self.processor.processed_df is not None:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ null ‡πÉ‡∏ô final dataset
            null_count = self.processor.processed_df.isnull().sum().sum()
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
            total_cells = self.processor.processed_df.shape[0] * self.processor.processed_df.shape[1]
            quality_score = ((total_cells - null_count) / total_cells) * 100 if total_cells > 0 else 0
            
            self.assertGreaterEqual(quality_score, 80.0)  # ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 80%
        
        print(f"‚úÖ PASS: Data Quality Framework integration - Score: {quality_score:.2f}%")
    
    def test_09_no_null_values_in_final_dataset(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ null values ‡πÉ‡∏ô final dataset"""
        print("test_no_null_values_in_final_dataset ... ", end="")
        
        if self.processor.processed_df is not None:
            null_count = self.processor.processed_df.isnull().sum().sum()
            self.assertEqual(null_count, 0, "Final dataset should not contain null values")
        
        print("‚úÖ PASS: ‡πÑ‡∏°‡πà‡∏°‡∏µ null values ‡πÉ‡∏ô final dataset")
    
    def test_10_data_types_correctness(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á data types"""
        print("test_data_types_correctness ... ", end="")
        
        if self.processor.processed_df is not None:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
            df = self.processor.processed_df
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö numeric columns
            numeric_columns = ['loan_amnt', 'funded_amnt', 'installment']
            for col in numeric_columns:
                if col in df.columns:
                    self.assertTrue(pd.api.types.is_numeric_dtype(df[col]))
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö datetime columns
            if 'issue_d' in df.columns:
                self.assertTrue(pd.api.types.is_datetime64_any_dtype(df['issue_d']))
        
        print("‚úÖ PASS: Data types ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î")
    
    def test_11_primary_keys_uniqueness(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö primary keys ‡πÉ‡∏ô dimension tables ‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥"""
        print("test_primary_keys_uniqueness ... ", end="")
        
        for dim_name, dim_table in self.processor.dimension_tables.items():
            pk_column = f'{dim_name}_id'
            if pk_column in dim_table.columns:
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ primary key ‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥
                unique_count = dim_table[pk_column].nunique()
                total_count = len(dim_table)
                self.assertEqual(unique_count, total_count, 
                               f"Primary key {pk_column} should be unique")
        
        print("‚úÖ PASS: Primary keys ‡πÉ‡∏ô dimension tables ‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥")
    
    def test_12_foreign_key_integrity(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö foreign key integrity"""
        print("test_foreign_key_integrity ... ", end="")
        
        if self.processor.fact_table is not None:
            fact_table = self.processor.fact_table
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö foreign key integrity
            for dim_name, dim_table in self.processor.dimension_tables.items():
                fk_column = f'{dim_name}_id'
                pk_column = f'{dim_name}_id'
                
                if fk_column in fact_table.columns and pk_column in dim_table.columns:
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ foreign key values ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô dimension table
                    fact_fk_values = set(fact_table[fk_column].dropna())
                    dim_pk_values = set(dim_table[pk_column])
                    
                    invalid_fks = fact_fk_values - dim_pk_values
                    self.assertEqual(len(invalid_fks), 0, 
                                   f"Invalid foreign keys found in {fk_column}")
        
        print("‚úÖ PASS: Foreign keys integrity ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
    
    def test_13_business_logic_validation(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ï‡∏£‡∏£‡∏Å‡∏∞‡∏ó‡∏≤‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à"""
        print("test_business_logic_validation ... ", end="")
        
        if self.processor.fact_table is not None:
            fact_table = self.processor.fact_table
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö loan amounts ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ö‡∏ß‡∏Å
            if 'loan_amnt' in fact_table.columns:
                self.assertTrue((fact_table['loan_amnt'] > 0).all(), 
                              "All loan amounts should be positive")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö funded_amount <= loan_amount
            if 'loan_amnt' in fact_table.columns and 'funded_amnt' in fact_table.columns:
                self.assertTrue((fact_table['funded_amnt'] <= fact_table['loan_amnt']).all(),
                              "Funded amount should not exceed loan amount")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö interest rate ‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏°‡∏ú‡∏• (0-1 for decimal format)
            if 'int_rate' in fact_table.columns:
                self.assertTrue((fact_table['int_rate'] >= 0).all() and 
                              (fact_table['int_rate'] <= 1).all(),
                              "Interest rates should be between 0 and 1")
        
        print("‚úÖ PASS: ‡∏ï‡∏£‡∏£‡∏Å‡∏∞‡∏ó‡∏≤‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
    
    def test_14_etl_processing_performance(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á ETL processing"""
        print("test_etl_processing_performance ... ", end="")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ß‡∏•‡∏≤‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
        processing_time = self.processing_result.processing_time
        
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ (‡πÄ‡∏ä‡πà‡∏ô ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 120 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)
        max_acceptable_time = 120.0
        self.assertLessEqual(processing_time, max_acceptable_time,
                           f"Processing time {processing_time:.2f}s exceeds {max_acceptable_time}s")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏î‡πâ
        self.assertGreater(self.processing_result.processed_records, 0,
                         "Should process at least some records")
        
        print(f"‚úÖ PASS: Performance OK - {processing_time:.2f}s for {self.processing_result.processed_records} records")
    
    def test_15_error_handling(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î"""
        print("test_error_handling ... ", end="")
        
        processor = ETLProcessor()
        
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
        result = processor.load_data('nonexistent_file.csv')
        self.assertFalse(result.success)
        self.assertGreater(len(result.errors), 0)
        
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å method ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        with self.assertRaises(ValueError):
            processor.filter_by_null_percentage()
        
        print("‚úÖ PASS: Error handling ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")


def main():
    """‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö ETL ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á"""
    
    # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ test suite
    suite = unittest.TestLoader().loadTestsFromTestCase(TestEnhancedETL)
    
    # ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    runner = unittest.TextTestRunner(verbosity=0, stream=open(os.devnull, 'w'))
    result = runner.run(suite)
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
    if result.wasSuccessful():
        print("\n‚úÖ All Enhanced ETL tests passed successfully!")
        return 0
    else:
        print(f"\n‚ùå {len(result.failures)} test(s) failed, {len(result.errors)} error(s)")
        for test, traceback in result.failures:
            print(f"FAIL: {test}")
            print(traceback)
        for test, traceback in result.errors:
            print(f"ERROR: {test}")
            print(traceback)
        return 1


if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)
