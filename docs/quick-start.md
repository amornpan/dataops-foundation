# üöÄ DataOps Foundation - Quick Start Guide

‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô DataOps Foundation ‡πÉ‡∏ô 15 ‡∏ô‡∏≤‡∏ó‡∏µ!

## üìã ‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô

- **Python 3.11+**
- **Git**
- **Docker & Docker Compose** (‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö)

## üöÄ ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß

### 1. Clone Repository

```bash
git clone https://github.com/amornpan/dataops-foundation.git
cd dataops-foundation
```

### 2. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Python Environment

```bash
# ‡∏™‡∏£‡πâ‡∏≤‡∏á virtual environment
python -m venv venv

# ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô virtual environment
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate

# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies
pip install -r requirements.txt
```

### 3. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô

```bash
# ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤
cp config/config.yaml config/config.local.yaml

# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö)
```

### 4. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á

```bash
# ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö ETL ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á
python tests/test_enhanced_etl.py

# ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢ pytest
pytest tests/ -v
```

## üí° ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô

### ETL Processing

```python
from src.data_pipeline.etl_processor import ETLProcessor

# ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô processor
processor = ETLProcessor()

# ‡∏£‡∏±‡∏ô ETL pipeline ‡πÅ‡∏ö‡∏ö end-to-end
result = processor.run_full_pipeline('data/input.csv')

if result.success:
    print(f"‚úÖ Processed {result.processed_records} records")
    print(f"üéØ Quality score: {result.quality_score}%")
else:
    print("‚ùå Pipeline failed:")
    for error in result.errors:
        print(f"   - {error}")
```

### Data Quality Checks

```python
from src.data_quality.quality_checker import DataQualityChecker
import pandas as pd

# ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
df = pd.read_csv('data/input.csv')

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
checker = DataQualityChecker()
quality_result = checker.run_checks(df)

# ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô
report = checker.generate_report(quality_result)
print(report)
```

## üê≥ ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Docker

### ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î

```bash
# ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£ DataOps Foundation
docker-compose -f docker/docker-compose.yml up -d

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
docker-compose -f docker/docker-compose.yml ps
```

### ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£

- **‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å**: http://localhost:8080
- **Jenkins CI/CD**: http://localhost:8081 (admin/DataOps123!)
- **Grafana Monitoring**: http://localhost:3000 (admin/DataOps123!)
- **Prometheus Metrics**: http://localhost:9090
- **Jupyter Notebooks**: http://localhost:8888 (token: DataOps123!)

## üîß ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á

### Environment Variables

```bash
# Database
export DATAOPS_DB_HOST=your-db-host
export DATAOPS_DB_USER=your-username
export DATAOPS_DB_PASSWORD=your-password

# Quality Thresholds
export DATAOPS_MAX_NULL_PCT=30
export DATAOPS_QUALITY_THRESHOLD=80

# Monitoring
export DATAOPS_ENABLE_METRICS=true
```

### Configuration File

‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç `config/config.yaml`:

```yaml
database:
  primary:
    host: "your-database-host"
    database: "your-database"
    username: "your-username"
    password: "your-password"

data_quality:
  max_null_percentage: 30.0
  quality_threshold: 80.0

monitoring:
  enabled: true
  prometheus:
    enabled: true
    port: 8000
```

## üìä ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° (Monitoring)

### Prometheus Metrics

DataOps Foundation ‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å metrics ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:

- `dataops_etl_records_processed_total` - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô records ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
- `dataops_etl_processing_duration_seconds` - ‡πÄ‡∏ß‡∏•‡∏≤‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
- `dataops_data_quality_score` - ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
- `dataops_etl_errors_total` - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î

### Grafana Dashboards

‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á Grafana ‡∏ó‡∏µ‡πà http://localhost:3000 ‡πÅ‡∏•‡∏∞ import dashboard ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏ß‡πâ‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤

## üß™ ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö

### ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î

```bash
# Unit tests
pytest tests/unit/ -v

# Integration tests  
pytest tests/integration/ -v

# Enhanced ETL tests
python tests/test_enhanced_etl.py

# Performance tests
pytest tests/performance/ -v --benchmark-only
```

### Coverage Report

```bash
pytest tests/ --cov=src --cov-report=html
# ‡πÄ‡∏õ‡∏¥‡∏î htmlcov/index.html ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô
```

## üöÄ CI/CD Pipeline

### Jenkins Setup

1. ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á Jenkins ‡∏ó‡∏µ‡πà http://localhost:8081
2. ‡∏™‡∏£‡πâ‡∏≤‡∏á Pipeline job ‡πÉ‡∏´‡∏°‡πà
3. ‡πÉ‡∏ä‡πâ `jenkins/Jenkinsfile` ‡πÄ‡∏õ‡πá‡∏ô pipeline script
4. Configure GitHub webhooks ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö auto-triggering

### Pipeline Stages

- **Code Quality Checks** - Linting, formatting, security scan
- **Unit Tests** - ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏û‡∏£‡πâ‡∏≠‡∏° coverage
- **Integration Tests** - ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏°‡∏£‡∏∞‡∏ö‡∏ö
- **Data Quality Tests** - ‡∏ó‡∏î‡∏™‡∏≠‡∏ö ETL ‡πÅ‡∏•‡∏∞ data quality
- **Performance Tests** - ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
- **Build Artifacts** - ‡∏™‡∏£‡πâ‡∏≤‡∏á deployment packages
- **Docker Build** - ‡∏™‡∏£‡πâ‡∏≤‡∏á container images

## üéØ Use Cases

### 1. ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• CSV ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà

```python
processor = ETLProcessor()

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
processor.max_null_percentage = 25
processor.acceptable_max_null = 20

# ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
result = processor.run_full_pipeline('large_dataset.csv')
```

### 2. ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö Real-time

```python
from src.monitoring.metrics_collector import MetricsCollector

collector = MetricsCollector()
checker = DataQualityChecker()

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å metrics
quality_result = checker.run_checks(df)
collector.record_data_quality_score(
    quality_result.overall_score, 
    'production_data', 
    'completeness'
)
```

### 3. ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Data Warehouse

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á dimensional model
processor.create_dimensional_model()

# ‡∏™‡∏£‡πâ‡∏≤‡∏á fact table
processor.create_fact_table()

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
processor.save_to_database()
```

## üÜò ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

```bash
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠
python -c "
from src.utils.config_manager import ConfigManager
config = ConfigManager()
print(config.get_connection_string())
"
```

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Performance

```bash
# ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô debug logging
export DATAOPS_LOG_LEVEL=DEBUG

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory usage
python -c "
import psutil
print(f'Memory: {psutil.virtual_memory().percent}%')
print(f'CPU: {psutil.cpu_percent()}%')
"
```

## üìö ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°

- **[Architecture Guide](docs/architecture.md)** - ‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°‡∏£‡∏∞‡∏ö‡∏ö
- **[API Reference](docs/api_reference.md)** - ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ API
- **[Deployment Guide](docs/deployment.md)** - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Production
- **[Contributing Guide](CONTRIBUTING.md)** - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤

## üí¨ ‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô

- üìã **Issues**: https://github.com/amornpan/dataops-foundation/issues
- üí¨ **Discussions**: https://github.com/amornpan/dataops-foundation/discussions
- üìß **Email**: dataops@company.com

---

**Happy DataOps! üéâ**
