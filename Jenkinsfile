pipeline {
    agent any
    
    environment {
        PYTHON_VERSION = '3.11'
        VIRTUAL_ENV = 'venv'
        PROJECT_NAME = 'dataops-foundation'
        DOCKER_IMAGE = 'dataops-foundation'
        DOCKER_TAG = "${BUILD_NUMBER}"
    }
    
    stages {
        stage('ğŸ—ï¸ Checkout') {
            steps {
                echo 'ğŸ“¥ Checking out source code...'
                checkout scm
                
                script {
                    // Get Git information
                    env.GIT_COMMIT_SHORT = sh(
                        script: 'git rev-parse --short HEAD',
                        returnStdout: true
                    ).trim()
                    
                    env.GIT_BRANCH = sh(
                        script: 'git rev-parse --abbrev-ref HEAD',
                        returnStdout: true
                    ).trim()
                }
                
                echo "ğŸ“‹ Build Information:"
                echo "   Git Commit: ${env.GIT_COMMIT_SHORT}"
                echo "   Git Branch: ${env.GIT_BRANCH}"
                echo "   Build Number: ${BUILD_NUMBER}"
                echo "   Jenkins URL: ${JENKINS_URL}"
                echo "   Workspace: ${WORKSPACE}"
            }
        }
        
        stage('ğŸ”§ Setup Environment') {
            steps {
                echo 'ğŸ Setting up Python environment...'
                
                // à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š Python version
                sh 'python3 --version'
                
                // à¸ªà¸£à¹‰à¸²à¸‡ virtual environment
                sh '''
                    python3 -m venv ${VIRTUAL_ENV}
                    . ${VIRTUAL_ENV}/bin/activate
                    pip install --upgrade pip
                    pip install wheel setuptools
                '''
                
                // à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡ dependencies
                sh '''
                    . ${VIRTUAL_ENV}/bin/activate
                    pip install -r requirements.txt
                    pip list
                '''
                
                // à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š project structure
                sh '''
                    echo "ğŸ“ Project Structure:"
                    find . -type f -name "*.py" | head -20
                    echo "..."
                    echo "Total Python files: $(find . -name "*.py" | wc -l)"
                '''
            }
        }
        
        stage('ğŸ“‹ Configuration Validation') {
            steps {
                echo 'âš™ï¸ Validating configuration...'
                
                sh '''
                    . ${VIRTUAL_ENV}/bin/activate
                    python -c "
import sys
import os
sys.path.insert(0, 'src')
from src.utils.config_manager import ConfigManager

print('ğŸ” Validating configuration...')
config = ConfigManager('config/config.yaml')
validation = config.validate_config()

print(f'Configuration Status: {'âœ… VALID' if validation['valid'] else 'âŒ INVALID'}')

if validation['errors']:
    print('âŒ Errors:')
    for error in validation['errors']:
        print(f'   - {error}')
        
if validation['warnings']:
    print('âš ï¸ Warnings:')
    for warning in validation['warnings']:
        print(f'   - {warning}')

if not validation['valid']:
    sys.exit(1)
"
                '''
            }
        }
        
        stage('ğŸ” Code Quality') {
            parallel {
                stage('ğŸ“ Linting') {
                    steps {
                        echo 'ğŸ” Running code linting...'
                        sh '''
                            . ${VIRTUAL_ENV}/bin/activate
                            
                            echo "Running flake8..."
                            flake8 src/ --count --select=E9,F63,F7,F82 --show-source --statistics || true
                            
                            echo "Running basic syntax check..."
                            python -m py_compile src/data_pipeline/etl_processor.py
                            python -m py_compile src/data_quality/quality_checker.py
                            python -m py_compile src/monitoring/metrics_collector.py
                            python -m py_compile src/utils/config_manager.py
                            python -m py_compile src/utils/logger.py
                        '''
                    }
                }
                
                stage('ğŸ”’ Security Scan') {
                    steps {
                        echo 'ğŸ”’ Running security scan...'
                        sh '''
                            . ${VIRTUAL_ENV}/bin/activate
                            
                            echo "Running basic security checks..."
                            python -c "
import os
import sys

# Check for common security issues
security_issues = []

# Check for hardcoded passwords
for root, dirs, files in os.walk('src'):
    for file in files:
        if file.endswith('.py'):
            filepath = os.path.join(root, file)
            try:
                with open(filepath, 'r') as f:
                    content = f.read()
                    if 'password' in content.lower() and ('=' in content or ':' in content):
                        if 'changeme' in content.lower() or 'password123' in content.lower():
                            security_issues.append(f'Potential hardcoded password in {filepath}')
            except:
                pass

if security_issues:
    print('ğŸ”’ Security Issues Found:')
    for issue in security_issues:
        print(f'   âš ï¸ {issue}')
else:
    print('âœ… No major security issues found')
"
                        '''
                    }
                }
            }
        }
        
        stage('ğŸ§ª Unit Tests') {
            steps {
                echo 'ğŸ§ª Running unit tests...'
                
                sh '''
                    . ${VIRTUAL_ENV}/bin/activate
                    
                    echo "ğŸ“Š Running test suite..."
                    python -m pytest tests/ -v --tb=short --disable-warnings || true
                    
                    echo "ğŸƒ Running enhanced tests..."
                    python tests/test_enhanced_etl.py || true
                '''
            }
            
            post {
                always {
                    // Collect test results
                    script {
                        if (fileExists('test-results.xml')) {
                            publishTestResults testResultsPattern: 'test-results.xml'
                        }
                    }
                }
            }
        }
        
        stage('ğŸ“Š Data Quality Tests') {
            steps {
                echo 'ğŸ“Š Running data quality tests...'
                
                sh '''
                    . ${VIRTUAL_ENV}/bin/activate
                    
                    echo "ğŸ­ Generating sample data..."
                    python -c "
import sys
import os
sys.path.insert(0, 'examples')
from generate_sample_data import generate_loan_data

print('Generating test data...')
df = generate_loan_data(1000, 'examples/sample_data/test_data.csv')
print(f'Generated {len(df)} records')
"
                    
                    echo "ğŸ” Testing data quality checks..."
                    python -c "
import sys
import os
import pandas as pd
sys.path.insert(0, 'src')
from src.data_quality.quality_checker import DataQualityChecker

print('Running data quality checks...')
if os.path.exists('examples/sample_data/test_data.csv'):
    df = pd.read_csv('examples/sample_data/test_data.csv')
    checker = DataQualityChecker()
    result = checker.run_checks(df)
    
    print(f'Quality Score: {result.overall_score:.1f}%')
    print(f'Grade: {result.grade}')
    print(f'Status: {'âœ… PASSED' if result.passed else 'âŒ FAILED'}')
    
    if result.overall_score < 70:
        print('âŒ Data quality below threshold!')
        sys.exit(1)
else:
    print('âš ï¸ Test data not found, skipping quality checks')
"
                '''
            }
        }
        
        stage('âš¡ Performance Tests') {
            steps {
                echo 'âš¡ Running performance tests...'
                
                sh '''
                    . ${VIRTUAL_ENV}/bin/activate
                    
                    echo "ğŸƒ Testing ETL performance..."
                    python -c "
import sys
import os
import time
import pandas as pd
import numpy as np
sys.path.insert(0, 'src')
from src.data_pipeline.etl_processor import ETLProcessor

print('Creating large test dataset...')
large_data = pd.DataFrame({
    'id': range(1, 5001),
    'value': np.random.randn(5000),
    'category': np.random.choice(['A', 'B', 'C'], 5000)
})
large_data.to_csv('temp_large_data.csv', index=False)

print('Testing ETL performance...')
processor = ETLProcessor()
start_time = time.time()
result = processor.load_data('temp_large_data.csv')
end_time = time.time()

duration = end_time - start_time
print(f'ETL Duration: {duration:.2f} seconds')
print(f'Records processed: {result.processed_records}')
print(f'Throughput: {result.processed_records/duration:.0f} records/second')

if duration > 30:
    print('âŒ ETL performance below threshold!')
    sys.exit(1)
else:
    print('âœ… ETL performance acceptable')

# Cleanup
os.remove('temp_large_data.csv')
"
                '''
            }
        }
        
        stage('ğŸ”„ Integration Tests') {
            steps {
                echo 'ğŸ”„ Running integration tests...'
                
                sh '''
                    . ${VIRTUAL_ENV}/bin/activate
                    
                    echo "ğŸ”— Testing component integration..."
                    python -c "
import sys
import os
sys.path.insert(0, 'src')
from src.utils.config_manager import ConfigManager
from src.data_pipeline.etl_processor import ETLProcessor
from src.data_quality.quality_checker import DataQualityChecker
from src.monitoring.metrics_collector import MetricsCollector

print('Testing configuration integration...')
config = ConfigManager('config/config.yaml')
validation = config.validate_config()

if not validation['valid']:
    print('âŒ Configuration validation failed!')
    sys.exit(1)

print('Testing ETL processor integration...')
processor = ETLProcessor('config/config.yaml')
print('âœ… ETL processor initialized')

print('Testing quality checker integration...')
checker = DataQualityChecker(config.config)
print('âœ… Quality checker initialized')

print('Testing metrics collector integration...')
collector = MetricsCollector(config.config)
print('âœ… Metrics collector initialized')

print('ğŸ‰ All components integrated successfully!')
"
                '''
            }
        }
        
        stage('ğŸ“¦ Build Artifacts') {
            steps {
                echo 'ğŸ“¦ Building deployment artifacts...'
                
                sh '''
                    . ${VIRTUAL_ENV}/bin/activate
                    
                    echo "ğŸ“‹ Creating build info..."
                    python -c "
import json
import os
from datetime import datetime

build_info = {
    'build_number': '${BUILD_NUMBER}',
    'git_commit': '${GIT_COMMIT_SHORT}',
    'git_branch': '${GIT_BRANCH}',
    'build_timestamp': datetime.now().isoformat(),
    'jenkins_url': '${JENKINS_URL}',
    'project_name': '${PROJECT_NAME}'
}

with open('build_info.json', 'w') as f:
    json.dump(build_info, f, indent=2)

print('Build info created')
"
                    
                    echo "ğŸ“Š Generating documentation..."
                    python -c "
import sys
import os
sys.path.insert(0, 'src')
from src.utils.config_manager import ConfigManager
from src.utils.version import get_version_info

print('Generating deployment documentation...')

# Get system info
version_info = get_version_info()
config = ConfigManager('config/config.yaml')
config_summary = config.get_config_summary()

doc_content = f'''
# DataOps Foundation Deployment Package

## Build Information
- Build Number: ${BUILD_NUMBER}
- Git Commit: ${GIT_COMMIT_SHORT}
- Git Branch: ${GIT_BRANCH}
- Build Date: $(date)

## Version Information
- Version: {version_info['version']}
- Build: {version_info['build']}

## Configuration Summary
- Config File: {config_summary['config_file']}
- Sections: {len(config_summary['sections'])}
- Total Keys: {config_summary['total_keys']}

## Deployment Instructions
1. Extract deployment package
2. Configure environment variables
3. Run: python main.py --mode info
4. Start services: python main.py --mode etl

## Health Check
- Endpoint: /health
- Expected Response: HTTP 200 with system status
'''

with open('DEPLOYMENT.md', 'w') as f:
    f.write(doc_content)

print('Documentation generated')
"
                '''
            }
        }
        
        stage('ğŸ³ Docker Build') {
            when {
                anyOf {
                    branch 'main'
                    branch 'master'
                    branch 'develop'
                }
            }
            steps {
                echo 'ğŸ³ Building Docker image...'
                
                script {
                    sh '''
                        echo "Building Docker image..."
                        docker build -f docker/Dockerfile -t ${DOCKER_IMAGE}:${DOCKER_TAG} .
                        docker build -f docker/Dockerfile -t ${DOCKER_IMAGE}:latest .
                        
                        echo "Docker image built successfully"
                        docker images | grep ${DOCKER_IMAGE}
                    '''
                }
            }
        }
        
        stage('ğŸš€ Deploy to Staging') {
            when {
                branch 'develop'
            }
            steps {
                echo 'ğŸš€ Deploying to staging environment...'
                
                sh '''
                    echo "Deploying to staging..."
                    
                    # Create staging deployment
                    mkdir -p staging_deployment
                    
                    # Copy essential files
                    cp -r src/ staging_deployment/
                    cp -r config/ staging_deployment/
                    cp -r examples/ staging_deployment/
                    cp requirements.txt staging_deployment/
                    cp main.py staging_deployment/
                    cp build_info.json staging_deployment/
                    cp DEPLOYMENT.md staging_deployment/
                    
                    # Create staging archive
                    tar -czf dataops-foundation-staging-${BUILD_NUMBER}.tar.gz staging_deployment/
                    
                    echo "Staging deployment package created"
                    ls -la *.tar.gz
                '''
            }
        }
        
        stage('ğŸ¯ Deploy to Production') {
            when {
                anyOf {
                    branch 'main'
                    branch 'master'
                }
            }
            steps {
                echo 'ğŸ¯ Deploying to production environment...'
                
                input message: 'Deploy to production?', ok: 'Deploy'
                
                sh '''
                    echo "Deploying to production..."
                    
                    # Create production deployment
                    mkdir -p production_deployment
                    
                    # Copy essential files
                    cp -r src/ production_deployment/
                    cp -r config/ production_deployment/
                    cp requirements.txt production_deployment/
                    cp main.py production_deployment/
                    cp build_info.json production_deployment/
                    cp DEPLOYMENT.md production_deployment/
                    
                    # Create production archive
                    tar -czf dataops-foundation-production-${BUILD_NUMBER}.tar.gz production_deployment/
                    
                    echo "Production deployment package created"
                    ls -la *.tar.gz
                '''
            }
        }
    }
    
    post {
        always {
            echo 'ğŸ§¹ Cleaning up workspace...'
            
            // Archive artifacts
            archiveArtifacts artifacts: 'build_info.json, DEPLOYMENT.md, *.tar.gz', allowEmptyArchive: true
            
            // Clean up temporary files
            sh '''
                rm -rf ${VIRTUAL_ENV}
                rm -rf staging_deployment
                rm -rf production_deployment
                rm -f temp_*.csv
                find . -name "*.pyc" -delete
                find . -name "__pycache__" -type d -exec rm -rf {} + 2>/dev/null || true
            '''
        }
        
        success {
            echo 'âœ… Pipeline completed successfully!'
            
            // Send success notification
            script {
                def buildInfo = """
ğŸ‰ **DataOps Foundation Build Success**

ğŸ“‹ **Build Information:**
- Build Number: ${BUILD_NUMBER}
- Git Commit: ${GIT_COMMIT_SHORT}
- Git Branch: ${GIT_BRANCH}
- Duration: ${currentBuild.durationString}

ğŸ¯ **Results:**
- All tests passed âœ…
- Code quality checks passed âœ…
- Deployment artifacts created âœ…

ğŸ”— **Links:**
- Build URL: ${BUILD_URL}
- Console Output: ${BUILD_URL}console
"""
                echo buildInfo
            }
        }
        
        failure {
            echo 'âŒ Pipeline failed!'
            
            // Send failure notification
            script {
                def buildInfo = """
âŒ **DataOps Foundation Build Failed**

ğŸ“‹ **Build Information:**
- Build Number: ${BUILD_NUMBER}
- Git Commit: ${GIT_COMMIT_SHORT}
- Git Branch: ${GIT_BRANCH}
- Duration: ${currentBuild.durationString}

ğŸ” **Investigation Required:**
- Check console output for details
- Review failed stage logs
- Verify code changes

ğŸ”— **Links:**
- Build URL: ${BUILD_URL}
- Console Output: ${BUILD_URL}console
"""
                echo buildInfo
            }
        }
    }
}
